{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f06068f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /opt/anaconda3/lib/python3.9/site-packages (0.6.0)\r\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/anaconda3/lib/python3.9/site-packages (from konlpy) (1.5.0)\r\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from konlpy) (4.9.1)\r\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/anaconda3/lib/python3.9/site-packages (from konlpy) (1.23.5)\r\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.9/site-packages (from JPype1>=0.7.0->konlpy) (22.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53e4f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '미국', 'value': 243}, {'text': '금리', 'value': 227}, {'text': '투자', 'value': 143}, {'text': '증권', 'value': 130}, {'text': '인하', 'value': 122}, {'text': '시장', 'value': 117}, {'text': '환율', 'value': 110}, {'text': '달러', 'value': 107}, {'text': '하락', 'value': 105}, {'text': '주가', 'value': 79}, {'text': '한국', 'value': 76}, {'text': '상승', 'value': 68}, {'text': '이후', 'value': 64}, {'text': '예상', 'value': 64}, {'text': '일본', 'value': 63}, {'text': '국내', 'value': 61}, {'text': '준', 'value': 61}, {'text': '트럼프', 'value': 59}, {'text': '정책', 'value': 58}, {'text': '이익', 'value': 57}]\n",
      "상위 20개 단어 빈도수 분석 결과가 'top_20_word_freq_USD.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "# 1. JSON 파일 불러오기\n",
    "with open('/Users/yang-jin-an/recent_usd_news.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 2. content를 하나의 텍스트로 저장\n",
    "all_text = ' '.join([article['content'] for article in data])\n",
    "\n",
    "# 3. 텍스트 전처리 및 형태소 분석\n",
    "okt = Okt()\n",
    "tokens = okt.nouns(all_text)  # 명사만 추출\n",
    "\n",
    "# 4. 불용어 리스트 정의\n",
    "stopwords = {'것','지수', '외화', '결제', '해외', '이번', '이번','이후', '준', \"예상\", '대비', '단지', '스토어', '명', '규모', '이상', '현지', '수',  '지난', '전망', '말', '시간', '로', '를', '의', '약', '은', '는', '이', '가' '수', '이', '등', '위', '경우', '때문', '및', '그', '더', '아이스크림','전', '고', '올해', '값', '때', '기준', '판매', '발생', '최근'}\n",
    "\n",
    "# 5. 불용어 제거\n",
    "filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "# 6. 단어 빈도수 검사 개발\n",
    "word_counts = Counter(filtered_tokens)\n",
    "\n",
    "# 7. 상위 20개 단어 추출\n",
    "top_20 = word_counts.most_common(20)\n",
    "result = [{'text': word, 'value': count} for word, count in top_20]\n",
    "\n",
    "# 결과 출력\n",
    "print(result)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('top_20_word_freq_USD.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False)\n",
    "\n",
    "print(\"상위 20개 단어 빈도수 분석 결과가 'top_20_word_freq_USD.json' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1235b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '케이', 'value': 1}, {'text': '케이 뱅크', 'value': 1}, {'text': '케이 뱅크 앱', 'value': 1}, {'text': '케이 뱅크 앱 외화', 'value': 1}, {'text': '케이 뱅크 앱 외화 환전', 'value': 1}, {'text': '해외', 'value': 1}, {'text': '해외 QR', 'value': 1}, {'text': '해외 QR 결제', 'value': 1}, {'text': '케이 뱅크 GLN', 'value': 1}, {'text': '케이 뱅크 GLN 인터내셔널', 'value': 1}, {'text': '케이 뱅크 GLN 인터내셔널 협업', 'value': 1}, {'text': '케이 뱅크 앱 해외', 'value': 1}, {'text': '케이 뱅크 앱 해외 결제', 'value': 1}, {'text': 'GLN', 'value': 1}, {'text': 'GLN 해외', 'value': 1}, {'text': 'GLN 해외 결제', 'value': 1}, {'text': '서비스', 'value': 1}, {'text': '서비스 출시', 'value': 1}, {'text': '20일', 'value': 1}, {'text': 'GLN 해외 결제 해외', 'value': 1}]\n",
      "상위 20개 단어 빈도수 분석 결과가 'top_20_word_freq_USD.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "# 1. JSON 파일 불러오기\n",
    "with open('/Users/yang-jin-an/recent_usd_news.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 2. content를 하나의 텍스트로 저장\n",
    "all_text = ' '.join([article['content'] for article in data])\n",
    "\n",
    "# 3. 텍스트 전처리 및 형태소 분석\n",
    "okt = Okt()\n",
    "pos_tags = okt.pos(all_text, norm=True, stem=True)\n",
    "\n",
    "# 4. 조사를 제거하고 단어 리스트 생성\n",
    "filtered_words = [word for word, tag in pos_tags if tag != 'Josa']\n",
    "\n",
    "# 5. 필터링된 단어들을 하나의 텍스트로 합침\n",
    "filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "# 6. 구(phrase) 추출\n",
    "tokens = okt.phrases(filtered_text)\n",
    "\n",
    "# 7. 불용어 리스트 정의\n",
    "stopwords = {'것', '이번', '대비', '단지', '스토어', '명', '규모', '이상', '현지', '수', '지난', '전망', '말', '시간', '로', '를', '의', '약', '은', '는', '이', '가', '등', '위', '경우', '때문', '및', '그', '더', '아이스크림', '전', '고', '올해', '값', '때', '기준', '판매', '발생', '최근'}\n",
    "\n",
    "# 8. 불용어 제거\n",
    "filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "# 9. 단어 빈도수 계산\n",
    "word_counts = Counter(filtered_tokens)\n",
    "\n",
    "# 10. 상위 20개 단어 추출\n",
    "top_20 = word_counts.most_common(20)\n",
    "result = [{'text': word, 'value': count} for word, count in top_20]\n",
    "\n",
    "# 결과 출력\n",
    "print(result)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('top_20_word_freq_USD.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False)\n",
    "\n",
    "print(\"상위 20개 단어 빈도수 분석 결과가 'top_20_word_freq_USD.json' 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d688200d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '미국', 'value': 0.08077598313775686}, {'text': '증권', 'value': 0.06943471721195778}, {'text': '금리', 'value': 0.0680896325779256}, {'text': '투자', 'value': 0.05884026751836967}, {'text': '하락', 'value': 0.052149600275827414}, {'text': '달러', 'value': 0.04787510670664212}, {'text': '인하', 'value': 0.04646363425782203}, {'text': '해외', 'value': 0.041481969558828435}, {'text': '주가', 'value': 0.03983461960842905}, {'text': '시장', 'value': 0.03911624391806741}, {'text': '결제', 'value': 0.03631198386095523}, {'text': '이익', 'value': 0.03605112127738388}, {'text': '영업', 'value': 0.03470735663357059}, {'text': '한국', 'value': 0.03465797651599328}, {'text': '투자자', 'value': 0.034033406235222406}, {'text': '원화', 'value': 0.03377946683317595}, {'text': '국내', 'value': 0.033157259518340115}, {'text': '지수', 'value': 0.03225100853311335}, {'text': '수익률', 'value': 0.0312222684826758}, {'text': '주식', 'value': 0.030298346595246915}]\n",
      "상위 20개 단어 TF-IDF 분석 결과가 'top_20_word_freq_USD.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. JSON 파일 불러오기\n",
    "with open('recent_usd_news.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 2. 모든 기사 내용 가져오기\n",
    "documents = [article['content'] for article in data]\n",
    "\n",
    "# 3. 불용어 리스트 정의\n",
    "stopwords = {'것', '이번', '대비', '단지', '스토어', '명', '규모', '이상', '현지', '수', '지난', '전망', '말', \n",
    "             '시간', '로', '를', '의', '약', '은', '는', '이', '가', '등', '위', '경우', '때문', '및', '그', '더', \n",
    "             '아이스크림', '전', '고', '올해', '값', '때', '기준', '판매', '발생', '최근', '으로', '에서', '이다', '있는'}\n",
    "\n",
    "# 4. 형태소 분석 및 전처리를 위한 함수 정의\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "def tokenize(text):\n",
    "    okt = Okt()\n",
    "    tokens = okt.nouns(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords and len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# 5. TF-IDF 벡터라이저 설정\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    max_features=1000,  # 상위 1000개의 단어만 사용\n",
    "    max_df=0.85,        # 전체 문서의 85% 이상에서 나타나는 단어는 무시\n",
    "    min_df=2            # 최소 2개 문서에 나타나는 단어만 사용\n",
    ")\n",
    "\n",
    "# 6. TF-IDF 행렬 생성\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 7. 전체 단어 리스트\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 8. 각 단어의 TF-IDF 평균 값 계산\n",
    "mean_tfidf = tfidf_matrix.mean(axis=0).tolist()[0]\n",
    "\n",
    "# 9. 단어와 TF-IDF 값을 딕셔너리로 묶기\n",
    "tfidf_scores = dict(zip(terms, mean_tfidf))\n",
    "\n",
    "# 10. TF-IDF 값에 따라 단어를 정렬하고 상위 20개 추출\n",
    "top_20 = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# 결과 출력\n",
    "result = [{'text': word, 'value': score} for word, score in top_20]\n",
    "print(result)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('top_20_word_freq_USD.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False)\n",
    "\n",
    "print(\"상위 20개 단어 TF-IDF 분석 결과가 'top_20_word_freq_USD.json' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a14a6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '달러', 'value': 1}, {'text': '달러 환율', 'value': 1}, {'text': '급락', 'value': 1}, {'text': '과거', 'value': 1}, {'text': '과거 원화값', 'value': 1}, {'text': '강세', 'value': 1}, {'text': '때 주가', 'value': 1}, {'text': '종목들', 'value': 1}, {'text': '주목', 'value': 1}, {'text': '20일', 'value': 1}, {'text': '20일 서울', 'value': 1}, {'text': '20일 서울 외환시장', 'value': 1}, {'text': '오전', 'value': 1}, {'text': '오전 11시15분', 'value': 1}, {'text': '오전 11시15분 현재', 'value': 1}, {'text': '오전 11시15분 현재 원', 'value': 1}, {'text': '전일', 'value': 1}, {'text': '전일 대비', 'value': 1}, {'text': '전일 대비 4.30원', 'value': 1}, {'text': '0.32%', 'value': 1}]\n",
      "상위 20개 복합 명사 빈도수 분석 결과가 'top_20_word_freq_USD.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "# 1. JSON 파일 불러오기\n",
    "with open('recent_usd_news.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 2. 모든 기사 내용 가져오기\n",
    "documents = [article['content'] for article in data]\n",
    "\n",
    "# 3. 모든 기사 내용을 하나의 문자열로 결합\n",
    "text = ' '.join(documents)\n",
    "\n",
    "# 4. 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 5. 복합 명사 추출\n",
    "tokens = okt.phrases(text)\n",
    "\n",
    "# 6. 불용어 리스트 정의\n",
    "stopwords = {'것', '이번', '대비', '단지', '스토어', '명', '규모', '이상', '현지', '수', '지난', \n",
    "             '전망', '말', '시간', '로', '를', '의', '약', '은', '는', '이', '가', '등', '위', \n",
    "             '경우', '때문', '및', '그', '더', '아이스크림', '전', '고', '올해', '값', '때', \n",
    "             '기준', '판매', '발생', '최근', '으로', '에서', '이다', '있는', '하지만', '또한', '하고', '했다'}\n",
    "\n",
    "# 7. 불용어 제거 및 길이가 2 이상인 단어만 추출\n",
    "tokens = [word for word in tokens if word not in stopwords and len(word) > 1]\n",
    "\n",
    "# 8. 단어 빈도수 계산\n",
    "word_counts = Counter(tokens)\n",
    "\n",
    "# 9. 상위 20개 단어 추출\n",
    "top_20 = word_counts.most_common(20)\n",
    "\n",
    "# 결과 출력\n",
    "result = [{'text': word, 'value': count} for word, count in top_20]\n",
    "print(result)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('top_20_word_freq_USD.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False)\n",
    "\n",
    "print(\"상위 20개 복합 명사 빈도수 분석 결과가 'top_20_word_freq_USD.json' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b985186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '금리 인하', 'value': 74}, {'text': '달러 환율', 'value': 45}, {'text': '달러 엔화', 'value': 23}, {'text': '엔화 약세', 'value': 23}, {'text': '미국 금리', 'value': 22}, {'text': '금리 인상', 'value': 19}, {'text': '기준금리 인하', 'value': 17}, {'text': '달러 원화', 'value': 17}, {'text': '국제 유가', 'value': 15}, {'text': '한국 경제', 'value': 15}, {'text': '상향 조정', 'value': 14}, {'text': '일본 정부', 'value': 14}, {'text': '엔화 가치', 'value': 14}, {'text': '해외 결제', 'value': 14}, {'text': '캐리 트레이드', 'value': 14}, {'text': '트럼프 대통령', 'value': 13}, {'text': '미국 연방', 'value': 13}, {'text': '밸류업 지수', 'value': 13}, {'text': '연방 준비', 'value': 12}, {'text': '인하 가능성', 'value': 12}]\n",
      "상위 20개 바이그램 빈도수 분석 결과가 'top_20_bigrams_USD.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "from itertools import tee\n",
    "\n",
    "# 1. JSON 파일 불러오기\n",
    "with open('recent_usd_news200.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 2. 모든 기사 내용 가져오기\n",
    "documents = [article['content'] for article in data]\n",
    "\n",
    "# 3. 모든 기사 내용을 하나의 문자열로 결합\n",
    "text = ' '.join(documents)\n",
    "\n",
    "# 4. 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 5. 명사 추출\n",
    "nouns = okt.nouns(text)\n",
    "\n",
    "# 6. 불용어 리스트 정의\n",
    "stopwords = {'것', '이번', '대비', '단지', '스토어', '명', '규모', '이상', '현지', '수', '지난', \n",
    "             '전망', '말', '시간', '로', '를', '의', '약', '은', '는', '이', '가', '등', '위', \n",
    "             '경우', '때문', '및', '그', '더', '아이스크림', '전', '고', '올해', '값', '때', \n",
    "             '기준', '판매', '발생', '최근', '으로', '에서','케이 뱅크', '하나 증권', '준비 제도',\n",
    "             '프로 골프', '여자 프로' '이다', '있는', '하지만', '또한', '하고', '했다', '증권 연구원',\n",
    "             '영업 이익', '투자 증권', '해외 결제', '이후', '여자', '영업', '골프', '연구원', '하나', '프로', '증권'}\n",
    "\n",
    "# 7. 불용어 제거 및 길이가 1자 이상인 단어만 추출\n",
    "nouns = [word for word in nouns if word not in stopwords and len(word) > 1]\n",
    "\n",
    "# 8. 2-그램(바이그램) 생성\n",
    "def ngrams(lst, n):\n",
    "    tlst = lst\n",
    "    while True:\n",
    "        a, b = tee(tlst)\n",
    "        l = tuple(next(a, None) for _ in range(n))\n",
    "        if None in l:\n",
    "            break\n",
    "        yield l\n",
    "        next(b)\n",
    "        tlst = b\n",
    "\n",
    "bigrams = list(ngrams(nouns, 2))\n",
    "\n",
    "# 9. 바이그램 빈도수 계산\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "# 10. 상위 20개 바이그램 추출\n",
    "top_20 = bigram_counts.most_common(20)\n",
    "\n",
    "# 결과 출력\n",
    "result = [{'text': ' '.join(words), 'value': count} for words, count in top_20]\n",
    "print(result)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('top_20_bigrams_USD_3.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False)\n",
    "\n",
    "print(\"상위 20개 바이그램 빈도수 분석 결과가 'top_20_bigrams_USD.json' 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a9e2fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '엔화 가치', 'value': 62}, {'text': '일본 증시', 'value': 57}, {'text': '금리 인상', 'value': 48}, {'text': '엔화 약세', 'value': 44}, {'text': '엔화 강세', 'value': 37}, {'text': '달러 엔화', 'value': 33}, {'text': '일본 주식', 'value': 27}, {'text': '일본 정부', 'value': 22}, {'text': '국내 투자자', 'value': 20}, {'text': '달러 환율', 'value': 19}, {'text': '개인 투자자', 'value': 19}, {'text': '일본 은행', 'value': 18}, {'text': '일본 중앙은행', 'value': 18}, {'text': '일본 기업', 'value': 18}, {'text': '상장 지수', 'value': 17}, {'text': '투자자 일본', 'value': 17}, {'text': '미국 금리', 'value': 17}, {'text': '지수 펀드', 'value': 16}, {'text': '미국 국채', 'value': 16}, {'text': '금리 인하', 'value': 16}]\n",
      "상위 20개 바이그램 빈도수 분석 결과가 'top_20_bigrams_USD.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "from itertools import tee\n",
    "\n",
    "# 1. JSON 파일 불러오기\n",
    "with open('recent_jpy_news200.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 2. 모든 기사 내용 가져오기\n",
    "documents = [article['content'] for article in data]\n",
    "\n",
    "# 3. 모든 기사 내용을 하나의 문자열로 결합\n",
    "text = ' '.join(documents)\n",
    "\n",
    "# 4. 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 5. 명사 추출\n",
    "nouns = okt.nouns(text)\n",
    "\n",
    "# 6. 불용어 리스트 정의\n",
    "stopwords = {'것', '이번', '대비', '단지', '스토어', '명', '규모', '이상', '현지', '수', '지난', \n",
    "             '전망', '말', '시간', '로', '를', '의', '약', '은', '는', '이', '가', '등', '위', \n",
    "             '경우', '때문', '및', '그', '더', '아이스크림', '전', '고', '올해', '값', '때', \n",
    "             '기준', '판매', '발생', '최근', '으로', '에서','케이 뱅크', '하나 증권', '준비 제도',\n",
    "             '프로 골프', '여자 프로' '이다', '있는', '하지만', '또한', '하고', '했다', '증권 연구원',\n",
    "             '영업 이익', '투자 증권', '해외 결제', '이후', '여자', '영업', '골프', '연구원', '하나', '프로', '증권',}\n",
    "\n",
    "# 7. 불용어 제거 및 길이가 1자 이상인 단어만 추출\n",
    "nouns = [word for word in nouns if word not in stopwords and len(word) > 1]\n",
    "\n",
    "# 8. 2-그램(바이그램) 생성\n",
    "def ngrams(lst, n):\n",
    "    tlst = lst\n",
    "    while True:\n",
    "        a, b = tee(tlst)\n",
    "        l = tuple(next(a, None) for _ in range(n))\n",
    "        if None in l:\n",
    "            break\n",
    "        yield l\n",
    "        next(b)\n",
    "        tlst = b\n",
    "\n",
    "bigrams = list(ngrams(nouns, 2))\n",
    "\n",
    "# 9. 바이그램 빈도수 계산\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "# 10. 상위 20개 바이그램 추출\n",
    "top_20 = bigram_counts.most_common(20)\n",
    "\n",
    "# 결과 출력\n",
    "result = [{'text': ' '.join(words), 'value': count} for words, count in top_20]\n",
    "print(result)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('top_20_bigrams_USD_3.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False)\n",
    "\n",
    "print(\"상위 20개 바이그램 빈도수 분석 결과가 'top_20_bigrams_USD.json' 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4e444d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('금리', '인하'), 74),\n",
       " (('투자', '증권'), 46),\n",
       " (('달러', '환율'), 45),\n",
       " (('증권', '연구원'), 37),\n",
       " (('영업', '이익'), 32),\n",
       " (('달러', '엔화'), 23),\n",
       " (('엔화', '약세'), 23),\n",
       " (('미국', '금리'), 22),\n",
       " (('금리', '인상'), 19),\n",
       " (('프로', '골프'), 19),\n",
       " (('기준금리', '인하'), 17),\n",
       " (('달러', '원화'), 17),\n",
       " (('국제', '유가'), 15),\n",
       " (('한국', '경제'), 15),\n",
       " (('여자', '프로'), 15),\n",
       " (('하나', '증권'), 14),\n",
       " (('상향', '조정'), 14),\n",
       " (('일본', '정부'), 14),\n",
       " (('이후', '개월'), 14),\n",
       " (('엔화', '가치'), 14),\n",
       " (('해외', '결제'), 14),\n",
       " (('캐리', '트레이드'), 14),\n",
       " (('트럼프', '대통령'), 13),\n",
       " (('미국', '연방'), 13),\n",
       " (('밸류업', '지수'), 13),\n",
       " (('미국', '여자'), 13),\n",
       " (('연방', '준비'), 12),\n",
       " (('인하', '가능성'), 12),\n",
       " (('미국', '국채'), 12),\n",
       " (('투자', '이민'), 12)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed71c838",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'soynlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msoynlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordExtractor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msoynlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnoun\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRNounExtractor_v2\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 1. JSON 파일 불러오기\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'soynlp'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "# 1. JSON 파일 불러오기\n",
    "with open('recent_usd_news.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 2. 모든 기사 내용 가져오기\n",
    "documents = [article['content'] for article in data]\n",
    "\n",
    "# 3. WordExtractor를 사용하여 단어 점수 계산\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(documents)\n",
    "word_score_table = word_extractor.extract()\n",
    "\n",
    "# 4. LRNounExtractor_v2를 사용하여 복합 명사 추출\n",
    "noun_extractor = LRNounExtractor_v2(verbose=True)\n",
    "nouns = noun_extractor.train_extract(documents)\n",
    "\n",
    "# 5. 추출된 명사 중 상위 20개 단어 추출\n",
    "sorted_nouns = sorted(nouns.items(), key=lambda x: x[1], reverse=True)\n",
    "top_20 = sorted_nouns[:20]\n",
    "\n",
    "# 결과 출력\n",
    "result = [{'text': word, 'value': score} for word, score in top_20]\n",
    "print(result)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('top_20_compound_nouns_USD.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False)\n",
    "\n",
    "print(\"상위 20개 복합 명사 빈도수 분석 결과가 'top_20_compound_nouns_USD.json' 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ad0ef48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '미국', 'value': 0.07138820757259516}, {'text': '금리', 'value': 0.0600830605077277}, {'text': '증권', 'value': 0.05951735407759274}, {'text': '투자', 'value': 0.053065728203041354}, {'text': '하락', 'value': 0.045662546298348206}, {'text': '달러', 'value': 0.0425047407694043}, {'text': '인하', 'value': 0.04202903193864075}, {'text': '해외', 'value': 0.036901033647687306}, {'text': '주가', 'value': 0.03614381582859805}, {'text': '시장', 'value': 0.03569231579491726}, {'text': '결제', 'value': 0.0325336552085979}, {'text': '한국', 'value': 0.03176361911643724}, {'text': '이익', 'value': 0.030786068441525972}, {'text': '따르다', 'value': 0.030143271043457782}, {'text': '투자자', 'value': 0.02987103751633181}, {'text': '원화', 'value': 0.029749522992130174}, {'text': '영업', 'value': 0.02947919228582165}, {'text': '국내', 'value': 0.029393780240085834}, {'text': '지수', 'value': 0.02881385777085248}, {'text': '수익률', 'value': 0.028339900587405046}]\n",
      "상위 20개 n-gram TF-IDF 분석 결과가 'top_20_word_freq_USD.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. JSON 파일 불러오기\n",
    "with open('recent_usd_news.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 2. 모든 기사 내용 가져오기\n",
    "documents = [article['content'] for article in data]\n",
    "\n",
    "# 3. 불용어 리스트 정의\n",
    "stopwords = {'것', '이번', '대비', '단지', '스토어', '명', '규모', '이상', '현지', '수', '지난', '전망', '말',\n",
    "             '시간', '로', '를', '의', '약', '은', '는', '이', '가', '등', '위', '경우', '때문', '및', '그', '더',\n",
    "             '아이스크림', '전', '고', '올해', '값', '때', '기준', '판매', '발생', '최근', '으로', '에서', '이다',\n",
    "             '있는', '하지만', '또한', '통해', '한다', '까지', '한', '및', '및', '하고', '보다', '있는'}\n",
    "\n",
    "# 4. 형태소 분석 및 전처리를 위한 함수 정의\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "def tokenize(text):\n",
    "    okt = Okt()\n",
    "    tokens = okt.pos(text, norm=True, stem=True)\n",
    "    words = []\n",
    "    for i in range(len(tokens)):\n",
    "        word, pos = tokens[i]\n",
    "        if pos in ['Noun', 'Adjective', 'Verb']:\n",
    "            if word not in stopwords and len(word) > 1:\n",
    "                words.append(word)\n",
    "    return words\n",
    "\n",
    "# 5. TfidfVectorizer 설정\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    ngram_range=(1, 3),  # unigram부터 trigram까지 사용\n",
    "    max_features=1000,   # 상위 1000개의 단어/구만 사용\n",
    "    max_df=0.85,         # 전체 문서의 85% 이상에서 나타나는 단어는 무시\n",
    "    min_df=2             # 최소 2개 문서에 나타나는 단어만 사용\n",
    ")\n",
    "\n",
    "# 6. TF-IDF 행렬 생성\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# 7. 전체 단어/구 리스트\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 8. 각 단어/구의 TF-IDF 평균 값 계산\n",
    "mean_tfidf = tfidf_matrix.mean(axis=0).tolist()[0]\n",
    "\n",
    "# 9. 단어/구와 TF-IDF 값을 딕셔너리로 묶기\n",
    "tfidf_scores = dict(zip(terms, mean_tfidf))\n",
    "\n",
    "# 10. TF-IDF 값에 따라 단어/구를 정렬하고 상위 20개 추출\n",
    "top_20 = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# 결과 출력\n",
    "result = [{'text': word, 'value': score} for word, score in top_20]\n",
    "print(result)\n",
    "\n",
    "# JSON 파일로 저장\n",
    "with open('top_20_word_freq_USD.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False)\n",
    "\n",
    "print(\"상위 20개 n-gram TF-IDF 분석 결과가 'top_20_word_freq_USD.json' 파일에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
